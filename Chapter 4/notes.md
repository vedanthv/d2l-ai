## Chapter 4 : Classification

**Softmax Function**
Assuming a suitable loss function, we could try, directly, to minimize the difference between and the labels. While it turns out that treating classification as a vector-valued regression problem works surprisingly well, it is nonetheless lacking in the following ways:

There is no guarantee that the outputs sum up to in the way we expect probabilities to behave.
There is no guarantee that the outputs are even nonnegative, even if their outputs sum up to ,or that they do not exceed.

Another way to accomplish this goal (and to ensure nonnegativity) is to use an exponential function 
```math
P(y = i) \propto \exp o_i
```
This does indeed satisfy the requirement that the conditional class probability increases with increasing, it is monotonic, and all probabilities are nonnegative. 

We can then transform these values so that they add up to 1 by dividing each by their sum. This process is called normalization.

```math
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o}) \quad \text{where}\quad \hat{y}_i = \frac{\exp(o_i)}{\sum_j \exp(o_j)}.
```

This formula demonstrates exponential normalization

We don't need to calculate the softmax for the function for which the probability is the highest

```math
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
```
**Vectorization**
We vectorize the calculations in batches of data.

```math
\begin{split}\begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned}\end{split}
```
This process of batching accelerates the calculation of XW. The complete operation can be calculated rowwise since each row represents one example.

**Loss Function**
The softmax function gives us y^ which can be estimated as the probabilities of each class. y1^ is essentially P(y = cat/x)

In the following we assume that for a dataset with features X and the labels Y are represented using a one-hot encoding label vector.

```math
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).
```

Let's take the negative log likelihood of the term so that finding the minima is easier. 

```math
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),
```

**Loss Function**
```math
l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.
```
The above function is called cross entropy loss.

For all but one value of q the cross entropy loss becomes 0.

There can be no entry with prob as 1 because then the error becomes infinity. The correspoding o value for t = 1 is inifinity

Plugging in log likelihood value in the loss function we get

```math
\begin{split}\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j \\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}\end{split}
```
Now calucate the derivative of above function with actually o value output

```math
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
```
In other words, the derivative is the difference between the probability assigned by our model, as expressed by the softmax operation, and what actually happened, as expressed by elements in the one-hot label vector. In this sense, it is very similar to what we saw in regression, where the gradient was the difference between the observation and estimate. 

This is not coincidence. In any exponential family model, the gradients of the log-likelihood are given by precisely this term. This fact makes computing gradients easy in practice.

**Information Theory Basics**

Deals with the encoding,decoding,transmitting and manipulating information.

1. Entropy - randomness in the data
```math
H[P] = \sum_j - P(j) \log P(j).
```

**Surprisal**

However if we cannot perfectly predict every event, then we might sometimes be surprised. Our surprise is greater when we assigned an event lower probability. 

Here is a formula that determines the probability that's lower than the true value.

```math
\log \frac{1}{P(j)} = -\log P(j)
```
This quantifies the surprisal of observing an event with probability P(j)

Cross Entropy is like a surpirse.
The cross-entropy from P to Q is denoted by H(P,Q) is the expected surprisal of an observer Q upon seeing data that was actually generated by probabilities of P.

```math
H(P, Q) \stackrel{\mathrm{def}}{=} \sum_j - P(j) \log Q(j)
```

The lowest possible Cross Entropy is when P = Q. The cross entropy is H(P,P) = H(P)

**Log Sum Exp Trick**
<img src = "D:\data-science\04-Deep Learning\D2L\images\logsumexp-1.JPG">

The value oj-o' is less than 0 for all j.

For a q class interval the denominator is in the range [1,j]

The numerator never exceeds 1 so we can ensure overflow is not there.

Nonetheless, a few steps down the road we might find ourselves in trouble when we want to compute log y as log 0. In particular, in backpropagation, we might find ourselves faced with a screenful of the dreaded NaN (Not a Number) results.

But since we take the log during cross entropy calculation, we divert this error.

```math
\log \hat{y}_j =
\log \frac{\exp(o_j - \bar{o})}{\sum_k \exp (o_k - \bar{o})} =
o_j - \bar{o} - \log \sum_k \exp (o_k - \bar{o}).
```
This avoids both overflow and underflow. We will want to keep the conventional softmax function handy in case we ever want to evaluate the output probabilities by our model.

**Generalization**

For any given model we can generate a priori

This priori specifies a error threshold epsilon that needs some n number of samples to keep the error below epsilon. 

But often the min value of n is very large so we use previous solutions on example solutions to come up with a post hoc analysis.

**Emperical Error on the Test Set**

```math
\epsilon_\mathcal{D}(f) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(f(\mathbf{x}^{(i)}) \neq y^{(i)}).
```

**Population Error**

The probability density function of the expected fraction of examples in the underlying population that is not of the correct labels.

```math
\epsilon(f) =  E_{(\mathbf{x}, y) \sim P} \mathbf{1}(f(\mathbf{x}) \neq y) =
\int\int \mathbf{1}(f(\mathbf{x}) \neq y) p(\mathbf{x}, y) \;d\mathbf{x} dy.
```

Since its not possible to consider the whole population, we consider only the sample from the test set and the population space error is equivalent to the sample error. 

An important classical result from probability theory called the central limit theorem guarantees that whenever we possess random samples a1,a2...an drawn from any distribution with mean <i>mu</i> and standard deviation mu/root(n), as the number of samples approaches infinity, the sample average approximately tends towards a normal distribution centered at the true mean and with standard deviation.

The sample error approaches the population error with a rate of O(1/root(n))

Variance is highest when the true error is 0.5 and lower when its close to 0 or 1.

If we want the test set error to be approximately equal to the population error, then we must take approximately 2500 samples if we want to fit the data in approximately 95% of std dev 1

If we want to fit two standard deviations with cf of 95% then we need 10000 samples.

**Test Set Reuse**

Let's say a model F1 is built and we need to test it.

Knowing just how confident you need to be in the performance of your classifier’s error rate you apply our analysis above to determine an appropriate number of examples to set aside for the test set.

 Finally you evaluate your model 
 on the test set and report an unbiased estimate of the population error with an associated confidence interval.

So far everything seems to be going well. However, that night you wake up at 3am with a brilliant idea for a new modeling approach. The next day, you code up your new model, tune its hyperparameters on the validation set and not only are you getting your new model F2 to work but it is error rate appears to be much lower than’s. However, the thrill of discovery suddenly fades as you prepare for the final evaluation. You do not have a test set!

When evaluating multiple models, ensuring that Err(Sample) + Err(Population) is atmost 0.01 is hard. Before on one model we could have been sure that this probability is 95% but with many classfiers in the mix, at least one of them may be a misleading score.

Once information from the test set has leaked to the modeler, it can never be a true test set again in the strictest sense. This problem is called adaptive overfitting and has recently emerged as a topic of intense interest to learning theorists and statisticians

We cant take multiple test sets for multiple hypothesis testing since the dataset may be small. 

**Statistical Learning Theory**

Test set tells us the post hoc analysis of testing but never about any priori that may tell us how to generalize.

The field of statistical learning theory aims to do exactly this.

In a sense the class of memorizers is too flexible. No such a uniform convergence result could possibly hold. On the other hand, a fixed classifier is useless—it generalizes perfectly, but fits neither the training data nor the test data. 

The central question of learning has thus historically been framed as a tradeoff between more flexible (higher variance) model classes that better fit the training data but risk overfitting, versus more rigid (higher bias) model classes that generalize well but risk underfitting. 

A central question in learning theory has been to develop the appropriate mathematical analysis to quantify where a model sits along this spectrum, and to provide the associated guarantees.

**Environmental And Distribution Shift**

There are two areas where Machine Learning Engineers do not concentrate on : 

- Where does the data for the models come from?
- What do we do with the conclusions that we arrive at from the mode?

Sometimes the ML Models perform very well on test dataset but fail in real deployed environments where the distribution of the model is different.

By not gauging the environment, we may break the model.

**Types of Distribution Shift**

**Covariate Shift**

The labels of a certain training set may change over time and this is called covariate shift. Covariate shift occurs due to change in the distribution of the dataset.

Eg : Train Data may consist of real cat and dog images whereas the test data may have cartoon images.

**Label Shift**

This is the opposite of Covariate Shift. We assume that P(y) can change but P(x/y) is constant and fixed. 

For example, we may want to predict diagnoses given their symptoms (or other manifestations), even as the relative prevalence of diagnoses are changing over time. Label shift is the appropriate assumption here because diseases cause symptoms.

**Concept Shift**

Diagnostic Criteria for mental illness, job titles and soft drink preferences change over time.

The distributions that gave rise to the training data and those you will encounter in the wild might differ considerably. This happened to an unfortunate startup that some of us (authors) worked with years ago. 


They were developing a blood test for a disease that predominantly affects older men and hoped to study it using blood samples that they had collected from patients. 


However, it is considerably more difficult to obtain blood samples from healthy men than sick patients already in the system. 


To compensate, the startup solicited blood donations from students on a university campus to serve as healthy controls in developing their test. Then they asked whether we could help them to build a classifier for detecting the disease.

**Self Driving Cars**

A similar thing happened to the US Army when they first tried to detect tanks in the forest. They took aerial photographs of the forest without tanks, then drove the tanks into the forest and took another set of pictures. The classifier appeared to work perfectly. 

Unfortunately, it had merely learned how to distinguish trees with shadows from trees without shadows—the first set of pictures was taken in the early morning, the second set at noon.

**Non Stationery Distributions**

- computational advertising model is non stationery.

- We build a spam filter. It works well at detecting all spam that we have seen so far. But then the spammers wisen up and craft new messages that look unlike anything we have seen before.

- We build a product recommendation system. It works throughout the winter but then continues to recommend Santa hats long after Christmas.

### Correcting Shift

**Empirical Risk**

Let’s first reflect about what exactly is happening during model training: we iterate over features and associated labels of training data 
``` math
\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}
```
Minimize Loss Function :

```math
\mathop{\mathrm{minimize}}_f \frac{1}{n} \sum_{i=1}^n l(f(\mathbf{x}_i), y_i),
```

The empirical risk is an average loss over the training data to approximate the risk, which is the expectation of the loss over the entire population of data drawn from their true distribution p(x,y)

**Concept Shift Correction**

Concept shift is much harder to fix in a principled manner. For instance, in a situation where suddenly the problem changes from distinguishing cats from dogs to one of distinguishing white from black animals, it will be unreasonable to assume that we can do much better than just collecting new labels and training from scratch. 

Fortunately, in practice, such extreme shifts are rare. Instead, what usually happens is that the task keeps on changing slowly. To make things more concrete, here are some examples:

- In computational advertising, new products are launched, old products become less popular. This means that the distribution over ads and their popularity changes gradually and any click-through rate predictor needs to change gradually with it.

- Traffic camera lenses degrade gradually due to environmental wear, affecting image quality progressively.

- News Content changes continuously.

### Types of Learning

1. Batch Learning

- We have access to training features which are used to build model f(x). THen we use this model to score another batch of data from the same distribution.

2. Online Learning

- Imagine that each (x,y) sample comes in one at a time. We come up with estimate f(xi) and then we can get output as yi.

- For example, we need to predict tomorrow’s stock price, this allows us to trade based on that estimate and at the end of the day we find out whether our estimate allowed us to make a profit.

3. Bandits

- In most ML Problems, we have a parameterized function f which can learn unlmited parameters, like a deep learning model.

- In a Bandit problem, we have a finite number of arms we can pull(a finite number of actions we can take)

4. Control

- In many cases the environment remembers what we did. Not necessarily in an adversarial manner but it will just remember and the response will depend on what happened before. 

- For instance, a coffee boiler controller will observe different temperatures depending on whether it was heating the boiler previously.

- Control Theory is used to tune hyper parameters to improve the reconstruction quality of user generated images and the diversity of generated text.

5. Reinforcement Learning

- In the more general case of an environment with memory, we may encounter situations where the environment is trying to cooperate with us (cooperative games, in particular for non-zero-sum games), or others where the environment will try to win.

- Chess and Go are games tht were crafted to leran from the actions that they take.

**Changing Algorithms Depending on the Environment**

- One key distinction between the different situations above is that the same strategy that might have worked throughout in the case of a stationary environment, might not work throughout when the environment can adapt.

- For instance, an arbitrage opportunity discovered by a trader is likely to disappear once he starts exploiting it. The speed and manner at which the environment changes determines to a large extent the type of algorithms that we can bring to bear.

- If the environment changes slowly then we can adapt the algorithms to adapt the change sloowly and vice versa.

**Why Accuracy isnt a good metric?**

Among other consequences of this change of scope, we will find that accuracy is seldom the right measure. For instance, when translating predictions into actions, we will often want to take into account the potential cost sensitivity of erring in various ways. 
 
If one way of misclassifying an image could be perceived as a racial sleight of hand, while misclassification to a different category would be harmless, then we might want to adjust our thresholds accordingly, accounting for societal values in designing the decision-making protocol.

**How Prediction System Changes according to Feedback Loops**

- Neighbourhoods with more crimes will get more police officers.

- Consequently, more crimes are discovered in these neighborhoods, entering the training data available for future iterations.

- In the next iteration, the updated model targets the same neighborhood even more heavily leading to yet more crimes discovered, etc.


